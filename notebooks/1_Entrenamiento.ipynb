{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10e08096",
   "metadata": {},
   "source": [
    "## 1) Creación de dataset entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4193e4",
   "metadata": {},
   "source": [
    "### Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607185fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importo librerias\n",
    "import glob\n",
    "import os\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d98cb04",
   "metadata": {},
   "source": [
    "### Instalacion de pysatproc (correr solo una vez)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e29c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip instal pysatproc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e289834e",
   "metadata": {},
   "source": [
    "### Variables para configuracion de satproc_extract_chips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60695bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dates=\"2021-01-01_2021-03-01\"                                      # nombre de la carpeta con las imagenes\n",
    "zona = \"zona_3\"                                                    # zona con la que vamos a trabajar\n",
    "version =  2                                                       # version de la corrida actual \n",
    "size = 100                                                         # tamaño (pixeles) de las imagenes que se van a generar\n",
    "step_size = 50                                                     # tamaño de la ventana\n",
    "vector_file_path = \"../data/shp/gt/R3/Asentamiento_R3_v2.geojson\"  # ruta y archivo con la verdad de campo\n",
    "\n",
    "###### estas variables no modifican #####\n",
    "path_to_files = os.path.join(\"../images-S2\",zona,dates,\"*.tif\")    # ruta a la carpeta con las imagenes satelitales\n",
    "vector_aoi_file_path = vector_file_path\n",
    "output_folder = os.path.join(\"../dataset/data_train\",zona,\"v\"+str(version),str(size)+\"_\"+str(step_size))         # Destination path <ndim>/<size>_<step-size>/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a858f8c",
   "metadata": {},
   "source": [
    "### Lista de imagenes que van a ser procesadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc991d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $path_to_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd344ec9",
   "metadata": {},
   "source": [
    "### Ayuda de satproc_extract_chips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0d4765",
   "metadata": {},
   "outputs": [],
   "source": [
    "#satproc_extract_chips --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1a5258",
   "metadata": {},
   "source": [
    "### Generacion del dataset para entrenamiento\n",
    "\n",
    "El modelo de ML utiliza para entrenar un dataset compuesto por imágenes y máscaras binarias que delimitan el objeto de interés. Estas se generan vía la herramienta **satproc_extract_chips**. Para el caso del dataset de entrenamiento se generan imágenes y máscaras (utilizando las anotaciones de verdad de campo) y para el de predicción solo imágenes.\n",
    "\n",
    "Para generar las imágenes de entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cc61e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creo el dataset para entrenamiento (imag + anotaciones)\n",
    "!satproc_extract_chips \\\n",
    "    $path_to_files \\\n",
    "    -o  $output_folder \\\n",
    "    --size $size \\\n",
    "    --step-size $step_size \\\n",
    "    --aoi $vector_file_path \\\n",
    "    --labels $vector_file_path \\\n",
    "    --label-property 'class' \\ # nombre del campo que tiene la clase a predecir\n",
    "    --classes 'A' \\            # valor que toma la clase a predecir\n",
    "    --rescale \\\n",
    "    --rescale-mode s2_rgb_extra --lower-cut 1 --upper-cut 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98882b2-4882-4c13-adbd-0912f121a77d",
   "metadata": {},
   "source": [
    "###### Los argumentos:\n",
    "\n",
    "* **El primer argumento** es la ruta a las imágenes \n",
    "\n",
    "* **o** es la ruta de destino \n",
    "\n",
    "Recomendamos que dicha ruta sea descriptiva, por ejemplo “data_train/600_600/ ” describe : Data_train → datos usados para entrenar; 400_400 → <tamaño de la imagen >_ <tamaño del step-size> (las imágenes son cuadradas)\n",
    "\n",
    "* **size** tamaño de las imágenes resultantes (las imágenes son cuadradas) \n",
    "* **step-size** paso del proceso. Si *step-size* es igual que el *size* entonces no hay overlap en las imágenes. \n",
    "\n",
    "En ocaciones es útil para el entrenamiento generar los chips con un overlap de este modo tenemos más datos para entrenar. Pero en la predicción valor debe ser igual al tamaño que la imagen \n",
    "\n",
    "* **crs** epsg: le asigna un epsg a la imagen \n",
    "\n",
    "* **label-property** nombre del campo donde se define cada categoría (solo se usa para el entrenamiento) \n",
    "\n",
    "* **classes** nombres de las clases (como aparecen en el geojson), separados por espacios\n",
    "\n",
    "* **aoi** ruta al archivo vectorial donde están definidas las localidades. Al definir una region de interés solo se procesan las imágenes que interceptan esas localidades.\n",
    "\n",
    "* **rescale** lleva los valores de las bandas a 0-255\n",
    "\n",
    "Este comando va a generar dos carpetas en la ruta de destino : “images” y “masks”. Los archivos de la primera van a ser de tipo Tiff de 3 bandas (rgb) y los de la segunda van a ser, también, de tipo Tiff pero de N bandas donde N representa el número de clases, en este caso sólo una. Y donde cada una de las bandas es una máscara binaria\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb62739",
   "metadata": {},
   "source": [
    "# 2) Entrenamiento\n",
    "\n",
    "Generamos el entrenamiento del modelo utilizando los datasets creados en el paso previo. \n",
    "El modelo es una red neuronal CNN basado en la arquitectura **U-Net**. Este considera las imágenes y las máscaras binarias como inputs y genera una imagen con la probabilidad de encontrar al objeto de interés.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e757608",
   "metadata": {},
   "source": [
    "### Instalacion de unet (correr solo una vez)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ea2652",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install unetseg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f201a1c4-bdad-44cf-8953-41a8d893603c",
   "metadata": {},
   "source": [
    "Las librerías que importamos nos permite acceder a las herramientas que vamos a utilizar a lo largo del proyecto como por ejemplo la función de train y predict  del modelo de Unet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da5d2d8",
   "metadata": {},
   "source": [
    "### Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470a0d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importo la red neuronal\n",
    "from unetseg.train import TrainConfig, train\n",
    "from unetseg.evaluate import plot_data_generator\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca2a820",
   "metadata": {},
   "source": [
    "### Variables para configuracion de unet\n",
    "\n",
    "En esta etapa debemos definir la configuración del modelo de ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5588f969",
   "metadata": {},
   "outputs": [],
   "source": [
    "spe=100          # step per epoch (debe ser multipo de 16 y estar entre 80 y 320)\n",
    "size_unet=160    # tamaño en la red (img/batch_size 10%)\n",
    "\n",
    "config = TrainConfig(width=size_unet,  \n",
    "                     height=size_unet, \n",
    "                     n_channels=4, \n",
    "                     n_classes=1,\n",
    "                     apply_image_augmentation=True,\n",
    "                     seed=42,\n",
    "                     epochs=30, \n",
    "                     batch_size=16, \n",
    "                     steps_per_epoch=spe, \n",
    "                     early_stopping_patience=10,\n",
    "                     validation_split=0.1, \n",
    "                     test_split=0.1,  \n",
    "                     model_architecture='unet', #unetplusplus\n",
    "                     images_path=os.path.join(\"../dataset/data_train\",zona,'v'+str(version),str(size)+\"_\"+str(step_size)),                     \n",
    "                     model_path=os.path.join('../data/weights/model',zona+'_UNet_techo_4D_spe'+str(spe)+'_img'+str(size)+'_size'+str(size)+'_sz'+str(step_size)+'.h5'),\n",
    "                     evaluate=True ,\n",
    "                     class_weights = [1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbddf07-c429-4c32-a63e-28b3ff69e690",
   "metadata": {},
   "source": [
    "*Obs:* Es util usar un nombre para el archivo de pesos que de informacion sobre los parametros de entrenamiento. por ejemplo: < modelo >_< proyecto >_< dim_de_las_imagenes >_< size >_< step_size >_< step_per_epoch >.h5 o similares "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f084a190",
   "metadata": {},
   "source": [
    "### Grafico de 3 tiles al azar\n",
    "Podemos visualizar alguna de las imágenes y máscaras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5433da4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_generator(num_samples=3, fig_size=(10, 10), train_config = config,img_ch = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f75327",
   "metadata": {},
   "source": [
    "### Corro el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6398f641",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Corro el modelo con los parametros establecidos\n",
    "res_config = train(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee38bac",
   "metadata": {},
   "source": [
    "### Graficos Loss y Mean_iou de train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92d256d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(16,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(res_config.history['loss'])\n",
    "plt.plot(res_config.history['val_loss'])\n",
    "plt.title('Loss')\n",
    "\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(res_config.history['mean_io_u'])\n",
    "plt.plot(res_config.history['val_mean_io_u'])\n",
    "plt.title('mean_iou')\n",
    "plt.ylabel('val_mean_iou')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
